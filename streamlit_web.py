import os
import streamlit as st
from http import HTTPStatus
from dashscope import Generation
import dashscope
from embedding_web import vectorize_and_store, convert_text, split_text
import time
from search_web import search_relevant_news
from pdf_topics_web import extract_pdf_topics, draw_knowledge_graph, load_document, visualize_text_processing, process_document
import speech_recognition as sr
import pyttsx3
import threading
from prompts import GovernmentAgentPrompts


# Define multi-round conversation function
def multi_round(messages, option, model, endpoint_api_key, endpoint_api_secret, collection_name):
    if option == "Default":
        response = Generation.call(
            model=model,
            messages=messages,
            result_format='message'
        )
        if response.status_code == HTTPStatus.OK:
            new_message = {
                'role': response.output.choices[0]['message']['role'],
                'content': response.output.choices[0]['message']['content']
            }
            messages.append(new_message)
            return messages, new_message
        else:
            st.error(f"Request id: {response.request_id}, Status code: {response.status_code}, "
                     f"error code: {response.code}, error message: {response.message}")
            return messages[:-1], None
    elif option == "RAG":
        question = messages[-1]["content"]
        best_match_news = search_relevant_news(question, endpoint_api_key, endpoint_api_secret, collection_name)
        response_message = answer_question(question, best_match_news, model)
        new_message = {
            'role': 'assistant',
            'content': response_message
        }
        messages.append(new_message)
        return messages, new_message


def answer_question(question, context, model):
    prompt = f'''è¯·åŸºäº```å†…çš„å†…å®¹å›ç­”é—®é¢˜ã€‚"
    ```
    {context}
    ```
    æˆ‘çš„é—®é¢˜æ˜¯ï¼š{question}ã€‚
    '''

    rsp = Generation.call(model=model,
                          prompt=prompt)
    if rsp.status_code == HTTPStatus.OK:
        return rsp.output.text.strip()
    else:
        st.error(f"Request id: {rsp.request_id}, Status code: {rsp.status_code}, "
                 f"error code: {rsp.code}, error message: {rsp.message}")
        return "Error"


# Real-time display function
def display_realtime_message(message_content, placeholder, role):
    full_text = ""
    for char in message_content:
        full_text += char
        with placeholder:
            st.chat_message(role).write(full_text)
        time.sleep(0.05)  # Control the display speed


# Sidebar design
def sidebar_configuration():
    st.sidebar.header("èµ„æ–™åº“ç®¡ç†")

    dashscope_api_key = st.sidebar.text_input("Dashscope API Key", key="chatbot_api_key", type="password")
    dashvector_api_key = st.sidebar.text_input("Dashvector API Key", key="chatbot_endpoint_api_key", type="password")
    dashvector_endpoint = st.sidebar.text_input("Dashvector Endpoint", key="chatbot_endpoint", type="password")
    # æ·»åŠ é“¾æ¥åˆ°ä¾§è¾¹æ 
    st.sidebar.markdown("[Get a Dashscope API key](https://dashscope.console.aliyun.com/)")
    st.sidebar.markdown("[View the source code](https://github.com/streamlit/llm-examples/blob/main/Chatbot.py)")
    st.sidebar.markdown(
        "[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/streamlit/llm-examples?quickstart=1)")
    pdf_folder_path = st.sidebar.text_input("èµ„æ–™æ¥æº", key="pdf_folder_path")

    pdf_files = list_pdf_files(pdf_folder_path)
    if pdf_files:
        st.sidebar.subheader("èµ„æ–™ ç›®å½•")
        for pdf_file in pdf_files:
            st.sidebar.markdown(f"- {pdf_file}")

    process_files_button = st.sidebar.button("è§£æ èµ„æ–™", key="process_button", help="å¤„ç†é€‰æ‹©çš„PDFæ–‡ä»¶",
                                             use_container_width=True)

    if process_files_button:
        st.session_state['show_modal'] = True

    if "chat_records" not in st.session_state:
        st.session_state["chat_records"] = []
        st.session_state["chat_titles"] = []

    if st.sidebar.button("æ–°å»ºèŠå¤©è®°å½•"):
        st.session_state["chat_records"].append([{"role": "assistant", "content": "æ‚¨å¥½ï¼æˆ‘æœ‰ä¹ˆèƒ½å¸®åˆ°æ‚¨ï¼Ÿ"}])
        st.session_state["chat_titles"].append("èŠå¤©è®°å½• " + str(len(st.session_state["chat_records"])))
        st.session_state["current_chat_index"] = len(st.session_state["chat_records"]) - 1

    for i, (record, title) in enumerate(zip(st.session_state["chat_records"], st.session_state["chat_titles"])):
        if st.sidebar.button(title, key=f"chat_record_{i}"):
            st.session_state["current_chat_index"] = i
            st.session_state["messages"] = record

    # åˆ é™¤ä»»åŠ¡ç±»å‹é€‰æ‹©ï¼Œä½¿ç”¨é»˜è®¤å€¼
    task_type = "é€šç”¨é—®ç­”"

    return dashscope_api_key, dashvector_api_key, dashvector_endpoint, pdf_folder_path, task_type


def list_pdf_files(pdf_folder_path):
    pdf_files = []
    if os.path.isdir(pdf_folder_path):
        for file_name in os.listdir(pdf_folder_path):
            if file_name.lower().endswith(".pdf"):
                pdf_files.append(file_name)
    return pdf_files


# Initialize conversation history
def initialize_messages():
    if "messages" not in st.session_state:
        st.session_state["messages"] = [{"role": "assistant", "content": "æ‚¨å¥½ï¼æˆ‘æœ‰ä»€ä¹ˆèƒ½å¸®åˆ°æ‚¨ï¼Ÿ"}]
    if "current_chat_index" not in st.session_state:
        st.session_state["current_chat_index"] = 0
        st.session_state["chat_records"].append(st.session_state["messages"])
        st.session_state["chat_titles"] = ["èŠå¤©è®°å½• 1"]


# Display conversation history
def display_messages():
    for msg in st.session_state.messages:
        if msg["role"] == "assistant":
            col1, col2, col3 = st.columns([3, 1, 1])
            with col1:
                st.chat_message(msg["role"]).write(msg["content"])
        else:
            col1, col2, col3 = st.columns([1, 1, 3])
            with col3:
                st.chat_message(msg["role"]).write(msg["content"])


# åœ¨æ–‡ä»¶å¼€å¤´æ·»åŠ ä»¥ä¸‹å¯¼å…¥è¯­å¥
import speech_recognition as sr
import pyttsx3
import threading

# åœ¨ main å‡½æ•°ä¹‹å‰æ·»åŠ ä»¥ä¸‹å‡½æ•°

def speech_to_text():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        st.write("è¯·è¯´è¯...")
        audio = recognizer.listen(source)
    try:
        text = recognizer.recognize_google(audio, language="zh-CN")
        return text
    except sr.UnknownValueError:
        st.error("æ— æ³•è¯†åˆ«è¯­éŸ³")
        return None
    except sr.RequestError:
        st.error("æ— æ³•è¿æ¥åˆ°è¯­éŸ³è¯†åˆ«æœåŠ¡")
        return None

def text_to_speech(text):
    engine = pyttsx3.init()
    engine.setProperty('rate', 150)
    engine.setProperty('voice', 'zh')
    engine.say(text)
    engine.runAndWait()

# Main application logic
def main():
    st.set_page_config(layout="wide")

    st.markdown(
        """
        <style>
        .small-selectbox select {
            width: 150px;
            font-size: 12px;
        }
        </style>
        """,
        unsafe_allow_html=True
    )

    col1, col2, col3 = st.columns([1, 1, 8])
    with col1:
        st.markdown('<div class="small-selectbox">', unsafe_allow_html=True)
        option = st.selectbox("é€‰æ‹©åŠŸèƒ½:", ("Default", "RAG"), key="function_select", index=0,
                              help="é€‰æ‹©ä½ éœ€è¦çš„åŠŸèƒ½ï¼ŒDefaultâ€”â€”æ­£å¸¸é—®ç­”ï¼›RAGâ€”â€”çŸ¥è¯†é—®ç­”")
        st.markdown('</div>', unsafe_allow_html=True)

    with col2:
        st.markdown('<div class="small-selectbox">', unsafe_allow_html=True)
        model = st.selectbox("é€‰æ‹©æ¨¡å‹:", [
            "qwen-turbo",
            "qwen-turbo-0624", "qwen-turbo-0206", "qwen-plus", "qwen-plus-0624", "qwen-plus-0206",
            "qwen-max", "qwen-max-0428", "qwen-max-0403", "qwen-max-0107", "qwen-max-1201", "qwen-max-longcontext",
            "qwen2-57b-a14b-instruct", "qwen2-72b-instruct", "qwen2-7b-instruct", "qwen2-1.5b-instruct",
            "qwen2-0.5b-instruct", "qwen1.5-110b-chat", "qwen1.5-72b-chat", "qwen1.5-32b-chat",
        ], key="model_select", index=0, help="é€‰æ‹©ä½ éœ€è¦çš„æ¨¡å‹")
        st.markdown('</div>', unsafe_allow_html=True)

    st.title("ğŸ’¬ Local Knowledge Quiz System")
    st.caption("ğŸš€ A Streamlit chatbot powered by Dashscope LLM")

    dashscope_api_key, dashvector_api_key, dashvector_endpoint, pdf_folder_path, task_type = sidebar_configuration()
    initialize_messages()

    chat_placeholder = st.container()

    with chat_placeholder:
        display_messages()

    if 'show_modal' not in st.session_state:
        st.session_state['show_modal'] = False

    if st.session_state['show_modal']:
        st.markdown("### Enter Collection Name")
        collection_name = st.text_input("Collection Name", key="collection_name_input")
        if st.button("Submit Collection Name"):
            st.session_state['collection_name'] = collection_name
            st.session_state['show_modal'] = False
            with st.spinner("æ­£åœ¨è§£ææ–‡çŒ®..."):
                result, topics = vectorize_and_store_and_extract_topics(dashscope_api_key, dashvector_api_key,
                                                                        dashvector_endpoint, pdf_folder_path,
                                                                        collection_name)
                if "successfully" in result:
                    st.success("æ–‡çŒ®å·²è§£æå®Œæ¯•ï¼Œå¯è¯¢é—®å…³äºæ–‡çŒ®é‡Œçš„çŸ¥è¯†ï¼")
                    st.session_state.messages = [
                        {"role": "assistant", "content": "æ‚¨å¥½ï¼æ–‡çŒ®å·²è§£æå®Œæ¯•ï¼Œå¯è¯¢é—®å…³äºæ–‡çŒ®é‡Œçš„çŸ¥è¯†ï¼"}]
                    st.session_state["topics"] = topics
                else:
                    st.error(result)

    if "topics" in st.session_state:
        # Create a right sidebar for displaying the knowledge graphs
        right_sidebar = st.sidebar.container()
        with right_sidebar:
            st.markdown("### æ–‡çŒ®çŸ¥è¯†å›¾è°±")
            for pdf_file, topics in st.session_state["topics"].items():
                st.markdown(f"**{pdf_file}**")
                draw_knowledge_graph(topics, pdf_file)

    # åœ¨ chat_placeholder ä¹‹åæ·»åŠ è¯­éŸ³è¾“å…¥æŒ‰é’®
    if st.button("è¯­éŸ³è¾“å…¥"):
        user_input = speech_to_text()
        if user_input:
            st.text_input("è¯­éŸ³è¾“å…¥ç»“æœ", value=user_input, key="speech_input")

    # ä¿®æ”¹ç°æœ‰çš„è¾“å…¥å¤„ç†é€»è¾‘
    prompt = st.chat_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜...") or st.session_state.get("speech_input", "")
    if prompt:
        if not dashscope_api_key:
            st.info("Please add your Dashscope API key to continue.")
            st.stop()

        dashscope.api_key = dashscope_api_key

        # ä½¿ç”¨æç¤ºç³»ç»Ÿç”Ÿæˆå®Œæ•´çš„æç¤ºï¼Œä½¿ç”¨é»˜è®¤çš„ä»»åŠ¡ç±»å‹
        full_prompt = GovernmentAgentPrompts.generate_response(task_type, prompt)
        
        user_message = {"role": "user", "content": prompt}  # åªå­˜å‚¨ç”¨æˆ·çš„åŸå§‹è¾“å…¥
        st.session_state.messages.append(user_message)
        with chat_placeholder:
            col1, col2, col3 = st.columns([1, 1, 9])
            with col3:
                st.chat_message(user_message["role"]).write(prompt)

        response_placeholder = st.empty()

        collection_name = st.session_state.get('collection_name', 'default_collection')
        _, response_message = multi_round([{"role": "user", "content": full_prompt}], option, model,
                                          dashvector_api_key, dashvector_endpoint,
                                          collection_name)

        if response_message:
            if response_message["role"] == "assistant":
                st.session_state.messages.append(response_message)
                display_realtime_message(response_message["content"], response_placeholder, response_message["role"])
                # æ·»åŠ è¯­éŸ³è¾“å‡º
                threading.Thread(target=text_to_speech, args=(response_message["content"],)).start()

        st.session_state.chat_records[st.session_state.current_chat_index] = st.session_state.messages

        if len(st.session_state.messages) > 1 and prompt != "":
            truncated_title = prompt[:16] + "..." if len(prompt) > 16 else prompt
            st.session_state.chat_titles[st.session_state.current_chat_index] = truncated_title

    # æ¸…é™¤è¯­éŸ³è¾“å…¥ç»“æœ
    if "speech_input" in st.session_state:
        del st.session_state.speech_input

    '''# æ·»åŠ æ–‡ä»¶ä¸Šä¼ åŠŸèƒ½
    uploaded_file = st.file_uploader("Choose a file", type=["pdf", "docx", "txt"])
    if uploaded_file is not None:
        text = load_document(uploaded_file)
        if text:
            st.success("æ–‡ä»¶åŠ è½½æˆåŠŸï¼")
            converted_text = convert_text(text)
            chunks = split_text(converted_text)
            visualize_text_processing(text, converted_text, chunks)'''

    # åœ¨main()å‡½æ•°ä¸­æ·»åŠ ä»¥ä¸‹ä»£ç 
    if st.checkbox("æ–‡æ¡£å¤„ç†å’Œå¯è§†åŒ–"):
        uploaded_file = st.file_uploader("é€‰æ‹©ä¸€ä¸ªæ–‡ä»¶", type=["pdf", "docx", "txt"])
        if uploaded_file is not None:
            text, chunks, structure = process_document(uploaded_file)
            if text:
                st.success("æ–‡ä»¶å¤„ç†æˆåŠŸï¼")
                
                # æ˜¾ç¤ºå¤„ç†åçš„æ–‡æœ¬å—
                if st.checkbox("æ˜¾ç¤ºå¤„ç†åçš„æ–‡æœ¬å—"):
                    for i, chunk in enumerate(chunks):
                        st.write(f"å— {i+1}:")
                        st.write(chunk)
                        st.write("---")
                
                # æ˜¾ç¤ºæå–çš„ç»“æ„
                if st.checkbox("æ˜¾ç¤ºæå–çš„æ–‡æ¡£ç»“æ„"):
                    st.json(structure)


def vectorize_and_store_and_extract_topics(dashscope_api_key, dashvector_api_key, dashvector_endpoint, pdf_folder_path,
                                           collection_name):
    result = vectorize_and_store(dashscope_api_key, dashvector_api_key, dashvector_endpoint, pdf_folder_path,
                                 collection_name)
    topics = extract_pdf_topics(pdf_folder_path)
    return result, topics


if __name__ == "__main__":
    main()